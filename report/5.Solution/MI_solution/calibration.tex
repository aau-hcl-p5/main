\section{Calibration}
As explained in \autoref{des:sec:variable_motor_power}, movement required different amount of power depending on the current position on each axis, and the direction of movement.


To solve this problem, data was gathered regarding the minimum amount of power.
This data was then processed in a neural network, finding a relation between power and angle in each direction.
This neural network was then compiled on the NXT, and used as a base-value for power.

To collect the data regarding the required power, a function was created on the NXT.
This function slowly increases power while checking for a change in the current revolution. 
A simplified version of the function can be seen in \autoref{lst:GetRequiredPower}.


\begin{lstlisting}[language=C,label={lst:GetRequiredPower},caption={Getting required power to move }]
int8_t get_power_to_move(T_AXIS_TYPE axis, T_DIRECTION direction) {
	int8_t power = MIN_POWER;
	T_REVOLUTION first_revolution = get_current_revolution();
	
	do {
		set_motor_speed(axis, power * (direction == POSITIVE ? 1 : -1));
		power++;
		
		// wait 30ms but make sure we don't move in that timeframe
		for(int i = 0; i < 30; i++)
		{
			systick_wait_ms(1);
			if (!should_stop_moving(first_revolution, power))
				break;
		}
	
	} while(should_stop_moving(first_revolution, power));
	
	set_motor_speed(axis, 0);
	
	return power;
}

\end{lstlisting}

The \texttt{should\_stop\_moving} function will determine whether the motor has moved, or if the power exceeds the maximum allowed power, meaning it is unable to move further.
The data is sent back to a host machine over the established USB connection.

The function was run multiple times, and whenever rebuilding parts of the machine, the calibration function was run again.

On of these data-sets, where the calibration process was run 5 times, can be seen in \autoref{fig:calib_data}.
This data-set is based on the y-axis, in an upwards direction.


\figur{1}{images/calib_data.png}{Required power for revolution on motor}{fig:calib_data}

There is a clear tendency in the data, even though it has some noise, which is presumably due to the quality of the motors.
The tendency seems reasonable regarding the upwards motion, where it initially requires low effort, but the requirement increases as it is reaching the top.

Using this data, a neural network was trained to find the tendency of the data.
The training was done with the Python module, scikit-learn\cite{scikit-learn}, which exposes a regressor called \texttt{MPLRegressor}, which was utilized as shown in \autoref{lst:mlpregressor}.
A custom modified version of the module was created, to be able to use a custom activation function.
This was done because of issues with the NXT-OSEK system, regarding the usage of the \texttt{math.h} library in C, which was bugged for some unknown reason.
Researching the bug did not give us any answers, and this seemed to be a known bug based on dialogue with other study groups.
Because of these issues, it was a requirement that custom implementations of math functions could be used, and a custom implementation of the \texttt{exp} function, was an unnecessary difficult task, so an approximation function was used instead.
\todo{Do we cover math.h issue anywhere else? we prolly should}

\autoref{lst:mlpregressor} shows how to train a multi layered neural network for regression.

\begin{lstlisting}[language=python,label={lst:mlpregressor},caption={Training a MLPRegressor with scikit}]
model = MLPRegressor(
hidden_layer_sizes=(30,),
activation='approx_sigmoid',
max_iter=100000,
tol=0.0000001,
verbose=True
)
model.fit(inp, expect)

\end{lstlisting}

The \texttt{MLPRegressor} has a lot of parameters that can be modified in order to tweak the neural network for your use case.
A custom amount of hidden layers was required as a the default argument is 100 neurons in one layer, which was not necessary for the rather simple data.
30 neurons in one layer was decided upon after testing different sizes.
A larger hidden layer did not improve the quality, however smaller networks seemed to decrease precision.

The system also utilized the custom implementation of the sigmoid function, and increased max iterations, and lowered the tolerance to make the network run for a longer time before stopping, to make sure the best possible network was reached.

One of the networks created with 30 neurons is shown in \autoref{fig:calib_data_nn}, where the line is the result of the neural network.
The data is normalized to better work with the SGD function, using a preprocessor exposed by the scikit-learn module.
Optimally a \textit{Gaussian distribution}, with zero mean and unit variance, would have been used, but due to the problems with \texttt{math.h}, this was not feasible.

\figur{1}{images/calib_data_nn.png}{Sample fitting of the data}{fig:calib_data_nn}

\autoref{fig:calib_data_nn} shows the neural network for the upwards direction, however the same process was done simultaneously for the downwards direction.
Calibration was only done for the $Y$-axis.
The two separate neural networks, produced at another calibration test, can be seen in \autoref{fig:calib_data_nn_both}.


\figur{1}{images/calib_data_nn_both.png}{Both neural networks of the $Y$ axis}{fig:calib_data_nn_both}

These neural networks had to be used to calculate the base value of a motion at a given angle at a given time.
To solve this, the weight and biases of the neural networks had to be exported to the NXT device.
This was done by generating the C code needed to replicate the behaviour.
A simplistic version of such a file can be seen in \autoref{lst:exported_model}.


\begin{lstlisting}[language=C,label={lst:exported_model},firstnumber={1},caption={Autogenerated model for getting power to move up}]
#include <stdint.h>
#include <math.h>

typedef struct {
	double input_0;
} T_MODEL_INPUT;

typedef struct {
	double output_0;
} T_MODEL_EXECUTION_RESULT;

static double sigmoid(double value)
{
	double x = value >= 0 ? value : value * -1;
	double x2 = x * x;
	double e = 1.0f + x + x2 * 0.555f + x2 * x2 * 0.143f;
	return 1.0f / (1.0f + (value > 0 ? 1.0f / e : e));
}
static double WEIGHTS_LAYER_0[1][30] = {
	{ 0.6808517232629588, .... , 4.170499471133788 }
};
static double WEIGHTS_LAYER_1[30][1] = {
	{ 0.8428497309705472 },
	...
	{ -2.015815865114775 }
};
static double BIAS_LAYER_0[30] = {
	-0.8555370213729661, ...., 3.3193827456697775
};
static double BIAS_LAYER_1[1] = {
	0.09092457039425662
};
T_MODEL_EXECUTION_RESULT calculate_model_up(T_MODEL_INPUT input) {
	//calculate result
}

\end{lstlisting}
The function \texttt{calculate\_model\_up} first normalizes the input data, and then calculates the output with matrix multiplication to calculate the output value, which is denormalized again.
This function is used in the movement module, to calculate the base power.
This is explained in detail in \autoref{sec:movement}.


