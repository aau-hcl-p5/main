\section{Structure of Codebase}
The codebase is split into two main parts, one part that is dedicated to object detection and the other part is for movement and interaction with the motors.
\todo{figure out whether the python part also does the calibration or not}
The object detection part, which is run on a Raspberry Pi, is written in Python 3{.}7, and the movement processing is handled by the NXT and is written i C, version C90 with GNU-extensions.


\subsection{Object Detection}
The codebase is seperated into modules with the intent of creating a system where each module can be exchanged based on different contexts and further development.
For the project, the following modules are used:
\begin{itemize}
	\item Object Localization Algorithm
	\item Output / Communication
	\item Input / Webcam
\end{itemize}

These are all utilized by a controller class, called \texttt{FlatController}, which ties the solution together, and has the signature shown in Snippet~\ref{lst:FlatControllerInit}.
\begin{lstlisting}[language=Python,label={lst:FlatControllerInit},caption={Initialization method of the \texttt{FlatController} class}]
def __init__(self,
		algorithm: Callable[[np.ndarray], Vector],
		output_device: OutputDevice,
		input_device: VideoController,
		calibration_algorithm: Union[Callable[[], None], None] = None,
	) -> None:
	"""
	Initializes the controller
	
	:param algorithm: The algorithm to use for image processing
	:param output_device: The device to send data to
	:param input_device: What type the capturing device should be.
	:param calibration_algorithm: a function that takes calibration packages,
	and handles them in some unknown way (either logs them or sends them back)
"""

\end{lstlisting}

The order of the parameters does not correspond to the order of relevance, but rather is a requirement as two of the parameters have default arguments.
This method takes, as the signature shows, a function, \texttt{algorithm} with an array as input and a vector as output.
This array is the array of pixels in a given frame.
The intent of this algorithm is to take a given frame, and localize the target within the frame.


\todo{implement the video output debug to actually be what i've written below}
The \texttt{output\_devices} parameter is a class that handles transmitting the location data to the NXT, but this can be overwritten to use a video feed as output as well, with the intent of debugging. 
This argument can either be a singular \texttt{OutputDevice} or multiple, which is relevant when the user both wants to debug but also transmit the information to the NXT device.


\todo{maybe change the capture\_type to also be either a class or a function, so we follow the same paradigm of coding all the way through}
Finally, the \texttt{capture\_type} parameter specifies which device the video feed should be received from.
By default, this should be a camera feed, but it might be relevant to use a specific video file in some contexts, for instance when doing tests.


The controller is initialized in the \texttt{main.py} file, which is the entry point when running the program. 
The relevant arguments are shown in Snippet~\ref{lst:MainHelp}.
\begin{lstlisting}[label={lst:MainHelp},caption={The help message of the commandline interface}]
$ python3 main.py --help
usage: main.py [-h] [-a [name]] [-d]

Run the object detection part of the F.L.A.T system

optional arguments:
-h, --help            
	show this help message and exit
-a [name], --algorithm [name] 
	Choose which algorithm to run [GOTURN, YOLO, ZONE_AVG, OBJ_FILL]. default='obj_fill'
-n [no_usb], --no-usb [no_usb]
	Whether to disable USB connection with the NXT.
	default=False
\end{lstlisting}
\todo{make sure these are still up to date when finishing the project, also image, bellow it isn't as Teknight created a new algorithm- but yeah}
Figure~\ref{fig:pythonClasses} shows the dependencies of each of the actual packages, and how each module is split up, into multiple packages or files.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{5.Solution/images/python_packages.png}
	\caption{The dependencies of the packages of the project{.} Image generated with pyreverse\cite{pyreverse}}
	\label{fig:pythonClasses}
\end{figure}


The project also contains functions for calibration, as seen on Figure~\ref{fig:pythonClasses}, which will be covered in Section~\ref{sec:calibration}.


\subsection{Movement}
The codebase is not in the same way seperated into modules, in a direct way, due to the design of the language.
 However the solution is divided into multiple file with distinct responsibilities.

\begin{itemize}
	\item \texttt{tasks}
	\item \texttt{usb}
	\item \texttt{display}
	\item \texttt{movement}
	\item \texttt{calibration}
\end{itemize}

the \texttt{tasks.c} can be seen as the primary file of the project, which creates all the tasks that the scheduling system enforces.
The code in snippet~\ref{lst:taskDecl} piece of code declares the tasks, but the specific aspects of each tasks, will be covered more indepth in section~\ref{sec:scheduling}
\begin{lstlisting}[language={c},label={lst:taskDecl},caption={Declaration of tasks, counters and events}]
/* OSEK declarations */
DeclareTask(ReceiveData);
DeclareTask(UpdateDisplay);
DeclareTask(ToggleLaser);
DeclareTask(MoveMotors);

DeclareCounter(SysTimerCnt);

DeclareEvent(MoveMotorsOnEvent);
DeclareEvent(MoveMotorsOffEvent);
DeclareEvent(LaserOnEvent);
DeclareEvent(LaserOffEvent);

DeclareResource(USB_Rx);
\end{lstlisting}
\todo{update above}
Rec


This file is not manually run, so less can be set about the overall principles of how it is run, as this is handled by the NXT-OSEK operating system, however the dependencies still seem relevant, which are detailed in figure~\ref{fig:CClasses}
\todo{insert picture here when we are done - i don't want to spend time creating one}


Each of these files and modules will be explained in-depth throughout this chapter, and explain value of each.

