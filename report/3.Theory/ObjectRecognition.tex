\section{Object Recognition}
To be able to figure out where the target is, its location within the video stream has to be determined.
It is not enough to simply answer the binary question "is a target present?", but rather "where is the target located?", so this information can be processed and in turn move the motors to point the laser at the target.
As mentioned in Section~\ref{sec:obj_tracking}, object recognition and localization algorithms can be tedious to implement by hand, and as a result of this, machine learning algorithms will be utilized.
Although the initial target of the \texttt{F.L.A.T} may be a simple and easily recognizable object, the intent is to create a solution that can identify more complex targets, in a potentially noisy image.
Because of this, it is important to look into the science of object identification, which algorithms and methods exists, and what libraries we have at our disposal.

It is important to mention that all mentions of object detection in the following subsections, refer to the localization of multiple objects.

\subsection{Neural Networks}

A neural network is, as the name implies, a network of neurons.
The idea is based on the neurons in the human brain, where the sum of the neurons are used to make more complex decisions than each individual neuron is capable of.
The primary difference is whether it's a biological brain or an artificial one. 
In computer science, a neuron can be seen as a function, that processes some input and then transmits the processed value to connected neurons.

Neural networks are a powerful but complex method of doing machine learning.
Because of this, simpler solutions should also be considered when exploring which methods suits the task best.
One of the reasons neural networks are especially interesting in this case, is its ability to recognize patterns.
Pattern recognition is a fundamental part of object recognition making neural networks an obvious option.


\subsubsection{Convolutional Neural Network}
\label{sec:cnn}
Convolutional neural networks (CNN) are often used for image classification, and is the most popular type of neural network in regards to object detection.
In challenges such as LSVCR\footnote{Imagenet Large Scale Visual Recognition Challenge} the majority of the entries are done using CNN \cite{ILSVRC_Results}.  
A CNN is a neural network with three categories of layers.
The layers are of the following types; an input layer, an output layer and a number of so called hidden layers.
The goal of the hidden layers is to transform the input data into the expected output data.
The CNN does this by doing abstractions within each layer, breaking a complex problem into smaller, more trivial, sub-problems.

An example of object recognition could be facial recognition.
The first layer might recognize lines, edges or similar low level features.
The following layers then, for example, detect increasingly complicated facial features, ending at some layer, in which one neuron would transmit a signal if a nose is present in the image, and another neuron to transmit if a mouth is present.


\figur{0.6}{images/cnn_face}{A simplified example of the abstractions within a CNN
	\cite{CNNStructure}.}{fig:face_cnn}

However, it is important to understand, that this is not necessarily the patterns that a CNN will detect within its layers.
Actually it is far more likely, that the layers will recognize something that to a human has no relation to a face, but the computer will have found a correlation between these seemingly random patterns, and a face.

CNNs are often used along with pooling.
Pooling is a concept where multiple outputs are combined into a single neuron in the next layer.
This can be done in order to down-sample the input and reduce the complexity, making the CNN run faster.
Some common examples of pooling is max pooling and average pooling, in which the average or max value is output from a number of input neurons.

CNNs are an effective choice for classification, meaning that it can answer whether the image is of a car.
Now the question is whether it can be used to figure out the position of the objects within the image.
The answer can be found by looking into Regional Convolutional Neural Network


\subsubsection{Regional Convolutional Neural Network}
A regional convolutional neural network (RCNN) utilizes CNN to get both a classification along with a bounding box, which in turn can be used to determine the center of the location.
It achieves this by applying the CNN to smaller regions of the image. 
These regions are extracted from the image using a method called selective search.
If the region contains an element recognized by the CNN, a bounding box can be drawn around the region.
The bounding box can finally be tightened using different methods, a popular option being a linear regression model.\cite{CNNHistory}

Unfortunately RCNN in itself is not viable for real time object detection.
However, there have been a lot of modifications and improvements of RCNN that improves the performance of it.
An improved version is called \textit{faster RCNN}, which is leaning towards being feasible for real time object detection, as it improves the performance of RCNN considerably\cite{fasterRCNN}.

\subsection{You Only Look Once}\label{sec:YOLOAnalysis}
An alternative to RCNN is You Only Look Once (YOLO) which is a new technology developed with real-time processing in mind.
Benchmarks shows, that YOLO can do real-time object detection using modern high-end consumer GPU's.
Furthermore the performance of YOLO beats that of \textit{faster RCNN}\cite{odDetection}.

YOLO also uses CNN like RCNN, but takes a fundamentally different approach to object detection.
YOLO will partition the image into $S\times S$ tiles, where each tile should determine $N$ bounding boxes, along with a confidence level for the given bounding box, signifying the confidence that a given bounding box actually contains an object.
YOLO will use a CNN in order to obtain a classification score for each bounding box.
The classifications scores along with the bounding box confidence is used to calculate the final probability score of a class being present in a given bounding box.\cite{odDetection}

As a result of this a total of $S\times S\times N$ tiles are given a probability score.
In order to only see the most relevant ones, a threshold should be set, so only the tiles with the highest probability is shown.

The process is visualized in Figure~\ref{fig:yolo}.

\figur{0.9}{images/yolo.jpg}{A visual representation of YOLO\cite{odDetection}.}{fig:yolo}

\subsection{GOTURN}
% https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/
Generic Object Tracking Using Regression Networks, is an algorithm based on deep learning\cite{goturn}.
GOTURN is different than the majority of algorithms presented in this section, in that it uses offline learning, which means training the model before execution time, whereas online learning constantly improves the model during run-time.
It learns about motion tracking and object identification, by utilizing a large set of videos of different objects moving.
The model is then shipped with the algorithm implementation, and therefore there is no need for online learning, making it more efficient.
The benefit of this, is that it runs faster, as it does not have to ever modify the model or analyze whether the given result is relevant.
However, this comes at a cost. 
The algorithm has a tendency to favor objects within the training set, even if they remain stationary.
For instance, if we want to track a bird flying past stationary planes, it might lock onto the planes rather than the bird, even though the planes are not moving, if the planes were in the training set.
This is due to the unpredictability of what the algorithm see as a correlation between video and a correct answer.


The GOTURN algorithm utilizes CNN, which is explained in Section~\ref{sec:cnn}.
\figur{0.9}{images/GOTURN-architecture.jpg}{How the algorithm utilizes CNN cite{goturn}.}{fig:goturn-arch}
GOTURN works by looking at two frames; the first, also known as the previous frame, and the second, also known as the current frame.
This is also shown on Figure~\ref{fig:goturn-arch}.
The premise is that the location of the object is known in the previous frame, and a bounding box is present.
Based on the bounding box of the previous frame, the current frame is cropped to two times the size of the previous bounding box.
A CNN is then trained to predict the bounding box in the second frame.

Some possible problems with this algorithm is that the location of the target is unknown initially, as the target will not necessarily be in the picture at startup time, and will eventually leave the image feed again, as the \texttt{F.L.A.T} will have idle time.
Another possible issue would be depending on the speed of the given target and the frame-rate of the camera, as the object might have left the cropped area. 
This could be solved by a larger crop, however this makes it less relevant.


\subsection{Tensorflow}
Tensorflow is a framework for working with machine learning\cite{tensorflow2015-whitepaper}.
The framework was initially developed for Python, but has recently implemented support for other languages.
It is developed by the Google Brain team, and owned by Google Inc.\cite{tensorflow-attribution}.
It has a high-level API, called Keras, and a low-level API, which makes it possible to use it both as at an entry-level, which makes it ideal for hobby and educational purposes, but it is also usable for complex production-level applications.

Tensorflow uses the concept of tensors to abstract the basis of computation, and makes it possible to write machine learning and machine intelligence with fewer steps.

A tensor is a mathematical entity, and the mathematical defintions are outside the scope of this project.
In the context of tensorflow, it is enough to think of it as an entity in a mathematical process.
This might be a constant primitive value, or a mapping from one or more primitive values to one or more primitive values.

To better show the example of tensors and how Tensorflow operates, the following low-level example is used in the tensorflow tutorial, and it seems relevant here as well.

\begin{lstlisting}[language=Python,label=lis:TensorFlowPy1,caption=Example of a tensorflow program]
a = tf.constant(3.0)
b = tf.constant(4.0)
total = a + b
print(a)
print(b)
print(total)
\end{lstlisting}
The reader might assume that the output would be 3,4, and 7, respectively, however the output is the following:
\begin{lstlisting}[language=Python,label=lis:TensorFlowPy2,caption=Output of example \ref{lis:TensorFlowPy2}]
Tensor("Const:0", shape=(), dtype=float32)
Tensor("Const_1:0", shape=(), dtype=float32)
Tensor("add:0", shape=(), dtype=float32)
\end{lstlisting}
This is because the $ a + b $ outputs another tensor, which can be evaluated outputting the result 7.
This is is because $a$ or $b$ might not be a constant, but a placeholder, which would be supplied at run-time, so building these tensors creates a model that can then be supplied data and then evaulated.

Tensors can also be tuples of different shapes, and when applying levels of abstraction, which is what Keras does, it is possible to use it for image recognition and other more complex problems.


\subsection{OpenCV}
OpenCV(Open Source Computer Vision Library), is a free-to-use library for C++, Python, and Java, which can be used for a lot of common computer vision tasks out of the box\cite{opencv}.

OpenCV is a popular library and also interfaces with other frameworks.
It is not a machine intelligence library, but rather a set of tools for manipulating images and doing general computer vision.
Its ability to interact with other frameworks along with all of its ready to use tools makes it very relevant when dealing with various computer vision tasks.


% Things to check / consider writing about
% https://en.wikipedia.org/wiki/Computer_vision#Recognition
% https://en.wikipedia.org/wiki/Video_tracking
% Kernel-based tracking
% Contour tracking
% https://en.wikipedia.org/wiki/Kalman_filter
% Particle filter
% https://en.wikipedia.org/wiki/Outline_of_object_recognition
% real-time: triangulate each frame and measure the persistence of selected triangles relative to their location in each successive frame. 
% Microprocessors such as the raspberry pi are fast enough so that triangulation and triangle measurement can be done in a few milliseconds.
% ^^ Few miliseconds (Sounds like maybe around 5ms, would alow for updating robot position 20 times a second (capture 20 fps from the camera too). 
% Which could be a goal to aim at (RTS: scheduling bla bla, update canon position 20 times pr second...))
% Would be cool in RTS part to integrate some functionality to handle if camera data is not ready within the 5 ms, or if the device is not ready to handle input after 5 ms. 
% The MI part could be coded to update each 5 ms.
